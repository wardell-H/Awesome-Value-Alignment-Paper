# 🌟 Awesome Value Alignment 🌟

欢迎来到 **Awesome Value Alignment** 仓库！这个收藏旨在整理和展示与 **人工智能的价值对齐** 相关的最具影响力和最新的研究论文、工具和资源。它旨在帮助研究人员、从业人员和爱好者更深入地探讨价值对齐的各个方面及其应用。

## 📚 目录
- [介绍](#介绍)
- [评估](#评估)
- [方法](#方法)
- [数据集](#数据集)
- [贡献](#贡献)
- [许可证](#许可证)

## 📖 介绍
价值对齐是人工智能研究中的一个关键领域，旨在确保 AI 系统的行为与人类的价值观和伦理相一致。本仓库将相关的关键论文和资源按主题进行分类，帮助社区更有效地探索并为该领域做出贡献。

## 🧑‍💻 评估
本部分包括专注于 **评估价值对齐** 方法及其有效性的论文和资源。



- **[论文标题 2](GitHub-Link)**
  - **作者**：Author1, Author2
  - **摘要**：简要总结论文的目标和贡献。
  - **GitHub** 💻：[链接](GitHub-Link)
  - **OpenReview** 📝：[链接](OpenReview-Link)
  - **arXiv** 📄：[链接](arXiv-Link)

## 🧠 方法
本部分包括介绍 **价值对齐** 新方法的论文和资源。

### 对原先RLHF进行改进 -- 需要一些最优化理论的基础
- **[(NeurIPS'25 Oral) Beyond Expectations: Quantile-Guided Alignment for Risk-Calibrated Language Models](GitHub-Link)**
  - **作者**：Xinran Wang, Jin Du
  - **摘要**：大型语言模型可能产生罕见但灾难性的输出，如有害的对话或不安全的代码。现有的人类反馈强化学习（RLHF）通常最大化平均回报，导致高风险尾部事件控制不足。我们引入了分位数引导对齐（QA），这是一个允许用户在任意分位数上指定期望改进——无论是单独还是跨多个奖励维度——从而将更细致控制的输出分布调整为更安全、更理想的结果。该方法通过增强奖励表述扩展标准RLHF，强制执行分位数约束。对话和代码生成任务的实验表明，分位数比对显著提升了目标尾部的质量，同时保持整体性能。结果将质量保证定位为一条原则性路径，实现风险校准语言模型，并以尾部为中心的对齐。
  - **OpenReview** 📝：[链接](https://openreview.net/forum?id=R7HJj1YvJH)

- **[(ICLR'25 Oral) MAP: Multi-Human-Value Alignment Palette](GitHub-Link)**
  - **作者**：Xinran Wang, Qi Le
  - **摘要**：确保生成式人工智能系统与人类价值观保持一致至关重要，但也极具挑战性，尤其是在考虑多种人类价值观及其潜在权衡取舍时。由于人类价值观可以个性化并随时间动态变化，因此不同种族、行业和用户群体对价值观一致性的理想水平也各不相同。在现有框架下，很难同时定义人类价值观并使人工智能系统在诸如无害性、助人性和积极性等不同方向上保持一致。为了解决这个问题，我们开发了一种名为“多人类价值观一致性调色板”（MAP）的新颖的第一性原理方法，它以结构化且可靠的方式实现多种人类价值观之间的一致性。MAP 将一致性问题建模为一个具有用户定义约束的优化任务，这些约束定义了人类价值观目标。它可以通过原始-对偶方法高效求解，该方法可以确定用户定义的一致性目标是否可行以及如何实现该目标。我们对MAP算法进行了详细的理论分析，量化了不同值之间的权衡、对约束的敏感性、多值对齐与序列对齐之间的根本联系，并证明了线性加权奖励足以实现多值对齐。大量实验表明，MAP算法能够以合理的方式对齐多个值，并在各种任务中都取得了优异的实证性能。。
  - **GitHub** 💻：[链接](https://github.com/wang8740/MAP)
  - **OpenReview** 📝：[链接](https://openreview.net/pdf?id=NN6QHwgRrQ)
  - **arXiv** 📄：[链接](https://arxiv.org/abs/2410.19198)

- **[论文标题 2](GitHub-Link)**
  - **作者**：Author1, Author2
  - **摘要**：简要总结论文的目标和贡献。
  - **GitHub** 💻：[链接](GitHub-Link)
  - **OpenReview** 📝：[链接](OpenReview-Link)
  - **arXiv** 📄：[链接](arXiv-Link)

## 🗂️ 数据集
本部分包括用于 **训练和测试** 价值对齐算法的数据集。

- **[数据集标题 1](GitHub-Link)**
  - **描述**：简要总结数据集的内容和使用场景。
  - **GitHub** 💻：[链接](GitHub-Link)
  - **OpenReview** 📝：[链接](OpenReview-Link)
  - **arXiv** 📄：[链接](arXiv-Link)

- **[数据集标题 2](GitHub-Link)**
  - **描述**：简要总结数据集的内容和使用场景。
  - **GitHub** 💻：[链接](GitHub-Link)
  - **OpenReview** 📝：[链接](OpenReview-Link)
  - **arXiv** 📄：[链接](arXiv-Link)

## 💡 贡献
我们欢迎对这个仓库的贡献！如果你发现有遗漏的论文或资源，或者希望提出改进建议，请随时打开 issue 或提交 pull request。

### 贡献步骤：
1. 🍴 Fork 该仓库。
2. 🔧 创建一个新分支。
3. 📑 在相应的部分添加你的论文/资源。
4. 🚀 提交 pull request，并简要描述你的新增内容。

## 📜 许可证
本项目采用 MIT 许可证 - 详情请参见 [LICENSE](LICENSE) 文件。

---

*在人工智能的旅程中，始终保持与人类价值观的对齐！* 🚀
