# Awesome Value Alignment

欢迎来到 **Awesome Value Alignment** 仓库！这个收藏旨在整理和展示与 **人工智能的价值对齐** 相关的最具影响力和最新的研究论文、工具和资源。它旨在帮助研究人员、从业人员和爱好者更深入地探讨价值对齐的各个方面及其应用。

## 目录
- [介绍](#介绍)
- [评估](#评估)
- [方法](#方法)
- [数据集](#数据集)
- [贡献](#贡献)
- [许可证](#许可证)

## 介绍
价值对齐是人工智能研究中的一个关键领域，旨在确保 AI 系统的行为与人类的价值观和伦理相一致。本仓库将相关的关键论文和资源按主题进行分类，帮助社区更有效地探索并为该领域做出贡献。

## 评估
本部分包括专注于 **评估价值对齐** 方法及其有效性的论文和资源。

- **[Beyond Expectations: Quantile-Guided Alignment for Risk-Calibrated Language Models](GitHub-Link)**
  - **作者**：Xinran Wang, Jin Du
  - **摘要**：大型语言模型可能产生罕见但灾难性的输出，如有害的对话或不安全的代码。现有的人类反馈强化学习（RLHF）通常最大化平均回报，导致高风险尾部事件控制不足。我们引入了分位数引导对齐（QA），这是一个允许用户在任意分位数上指定期望改进——无论是单独还是跨多个奖励维度——从而将更细致控制的输出分布调整为更安全、更理想的结果。该方法通过增强奖励表述扩展标准RLHF，强制执行分位数约束。对话和代码生成任务的实验表明，分位数比对显著提升了目标尾部的质量，同时保持整体性能。结果将质量保证定位为一条原则性路径，实现风险校准语言模型，并以尾部为中心的对齐。。
  - **OpenReview**：[链接](https://openreview.net/forum?id=R7HJj1YvJH)

- **[论文标题 2](GitHub-Link)**
  - **作者**：Author1, Author2
  - **摘要**：简要总结论文的目标和贡献。
  - **GitHub**：[链接](GitHub-Link)
  - **OpenReview**：[链接](OpenReview-Link)
  - **arXiv**：[链接](arXiv-Link)

## 方法
本部分包括介绍 **价值对齐** 新方法的论文和资源。

- **[论文标题 1](GitHub-Link)**
  - **作者**：Author1, Author2
  - **摘要**：简要总结论文的目标和贡献。
  - **GitHub**：[链接](GitHub-Link)
  - **OpenReview**：[链接](OpenReview-Link)
  - **arXiv**：[链接](arXiv-Link)

- **[论文标题 2](GitHub-Link)**
  - **作者**：Author1, Author2
  - **摘要**：简要总结论文的目标和贡献。
  - **GitHub**：[链接](GitHub-Link)
  - **OpenReview**：[链接](OpenReview-Link)
  - **arXiv**：[链接](arXiv-Link)

## 数据集
本部分包括用于 **训练和测试** 价值对齐算法的数据集。

- **[数据集标题 1](GitHub-Link)**
  - **描述**：简要总结数据集的内容和使用场景。
  - **GitHub**：[链接](GitHub-Link)
  - **OpenReview**：[链接](OpenReview-Link)
  - **arXiv**：[链接](arXiv-Link)

- **[数据集标题 2](GitHub-Link)**
  - **描述**：简要总结数据集的内容和使用场景。
  - **GitHub**：[链接](GitHub-Link)
  - **OpenReview**：[链接](OpenReview-Link)
  - **arXiv**：[链接](arXiv-Link)

## 贡献
我们欢迎对这个仓库的贡献！如果你发现有遗漏的论文或资源，或者希望提出改进建议，请随时打开 issue 或提交 pull request。

### 贡献步骤：
1. Fork 该仓库。
2. 创建一个新分支。
3. 在相应的部分添加你的论文/资源。
4. 提交 pull request，并简要描述你的新增内容。

## 许可证
本项目采用 MIT 许可证 - 详情请参见 [LICENSE](LICENSE) 文件。

---

*在人工智能的旅程中，始终保持与人类价值观的对齐！*
